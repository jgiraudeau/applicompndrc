import os
import google.generativeai as genai
from dotenv import load_dotenv

def test_gemini_connection():
    # 1. Load Environment Variables
    load_dotenv()
    
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("âŒ ERREUR: La clÃ© API 'GOOGLE_API_KEY' est introuvable dans le fichier .env")
        print("â¡ï¸  Veuillez crÃ©er un fichier .env dans le dossier backend avec votre clÃ©.")
        return

    print(f"âœ… ClÃ© API trouvÃ©e: {api_key[:5]}...*****")

    # 2. Configure Gemini
    try:
        genai.configure(api_key=api_key)
        print("âœ… Configuration Gemini OK.")
    except Exception as e:
        print(f"âŒ Erreur de configuration: {e}")
        return

    # 3. List Available Models
    print("\nğŸ” Recherche des modÃ¨les disponibles...")
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                print(f"   - {m.name}")
    except Exception as e:
        print(f"âŒ Erreur lors du listing des modÃ¨les : {e}")

    # 4. Robust Test Generation
    print("\nğŸ“¡ Test de connexion en cours (GÃ©nÃ©ration de texte)...")
    
    # 4. Smart Auto-Discovery Test
    print("\nğŸ“¡ Test de connexion en cours (Mode Auto-DÃ©couverte)...")
    
    available_models = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                available_models.append(m.name)
    except Exception as e:
        print(f"âŒ Impossible de lister les modÃ¨les pour l'auto-dÃ©couverte : {e}")
        return

    if not available_models:
        print("âŒ Aucun modÃ¨le compatible 'generateContent' trouvÃ©.")
        return
        
    print(f"â„¹ï¸ {len(available_models)} modÃ¨les compatibles trouvÃ©s. Test du premier disponible...")

    # Sort to try 'flash' or 'pro' first if available, otherwise take the first one
    # This prefers models with shorter names (usually stable versions)
    available_models.sort(key=lambda x: len(x)) 
    
    success = False
    for model_name in available_models:
        # We prioritize flash models for speed in this test
        print(f"ğŸ‘‰ Tentative avec : {model_name}...")
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content("Dis bonjour au Professeur Virtuel en une phrase.")
            print(f"\nğŸ¤– RÃ©ponse de Gemini ({model_name}) :\n> {response.text}")
            print("\nğŸ‰ SUCCÃˆS : La connexion Ã  Gemini est opÃ©rationnelle !")
            success = True
            break
        except Exception as e:
            print(f"   âš ï¸ Ã‰chec : {str(e)[:100]}...")

    if not success:
         print("\nâŒ TOUS les tests ont Ã©chouÃ©. VÃ©rifiez votre clÃ© API ou les quotas.")
         return

    # 5. Test File Upload & Comprehension
    print("\nğŸ“„ Test d'analyse de document (RAG Native)...")
    file_path = "conception_systeme.md"
    
    if not os.path.exists(file_path):
        print(f"âš ï¸ Fichier non trouvÃ© : {file_path}")
        return

    print(f"ğŸ‘‰ Upload de {file_path}...")
    try:
        # Upload file with explicit mime type
        uploaded_file = genai.upload_file(file_path, mime_type="text/markdown")
        print(f"   ID Fichier: {uploaded_file.name}")
        
        # Wait for processing
        import time
        while uploaded_file.state.name == "PROCESSING":
            print("   â³ Traitement en cours...")
            time.sleep(2)
            uploaded_file = genai.get_file(uploaded_file.name)
            
        if uploaded_file.state.name == "FAILED":
             print("âŒ L'upload a Ã©chouÃ©.")
             return

        print("   âœ… Fichier prÃªt.")

        # Ask question
        print("ğŸ‘‰ Question: 'Quels sont les modules principaux du projet ?'")
        
        # Force use of a Multimodal model (Flash is best for this)
        # We look for 'flash' in the available models we found earlier
        pro_model_name = next((m for m in available_models if "flash" in m), None)
        
        if not pro_model_name:
             # Fallback to any 1.5 model
             pro_model_name = next((m for m in available_models if "1.5" in m), available_models[0])

        print(f"ğŸ‘‰ Utilisation du modÃ¨le Multimodal : {pro_model_name}")
        
        model = genai.GenerativeModel(pro_model_name)
        
        response = model.generate_content(
            [uploaded_file, "Quels sont les modules principaux dÃ©crit dans ce document ? Fais une liste Ã  puces."]
        )
        print(f"\nğŸ¤– RÃ©ponse de Gemini :\n{response.text}")
        print("\nğŸ‰ SUCCÃˆS : Gemini a lu et compris le fichier !")
        
    except Exception as e:
        print(f"\nâŒ Ã‰CHEC du test de fichier : {e}")

if __name__ == "__main__":
    test_gemini_connection()


if __name__ == "__main__":
    test_gemini_connection()
